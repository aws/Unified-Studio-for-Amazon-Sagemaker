"""Integration test for multi-target pipeline workflow."""

import pytest
import os
from typer.testing import CliRunner
from ..base import IntegrationTestBase
from smus_cicd.helpers.utils import get_datazone_project_info


class TestMultiTargetApp(IntegrationTestBase):
    """Test multi-target pipeline end-to-end workflow."""

    def setup_method(self, method):
        """Set up test environment."""
        super().setup_method(method)
        self.setup_test_directory()
        self.cleanup_glue_databases()

    def cleanup_glue_databases(self):
        """Delete Glue databases that might conflict with the test."""
        import boto3
        
        glue_client = boto3.client('glue', region_name='us-east-1')
        lakeformation_client = boto3.client('lakeformation', region_name='us-east-1')
        sts_client = boto3.client('sts', region_name='us-east-1')
        
        # Get current role ARN
        identity = sts_client.get_caller_identity()
        role_arn = identity['Arn']
        print(f"‚úÖ AWS credentials verified: {role_arn}")
        
        databases_to_delete = [
            'multi_target_airless_test_db',
            'multi_target_airless_prod_db'
        ]
        
        for db_name in databases_to_delete:
            try:
                # First, try to grant Lake Formation DROP permission
                try:
                    lakeformation_client.grant_permissions(
                        Principal={'DataLakePrincipalIdentifier': role_arn},
                        Resource={'Database': {'Name': db_name}},
                        Permissions=['DROP']
                    )
                    print(f"‚úÖ Granted Lake Formation DROP permission for {db_name}")
                except Exception as perm_error:
                    if "Database not found" in str(perm_error):
                        print(f"‚ÑπÔ∏è  Database {db_name} doesn't exist, no permissions needed")
                    else:
                        print(f"‚ö†Ô∏è  Could not grant permissions for {db_name}: {perm_error}")
                
                # Now try to delete the database
                glue_client.delete_database(Name=db_name)
                print(f"‚úÖ Deleted Glue database: {db_name}")
            except glue_client.exceptions.EntityNotFoundException:
                print(f"‚ÑπÔ∏è  Glue database {db_name} does not exist (OK)")
            except Exception as e:
                error_msg = str(e)
                if "AccessDenied" in error_msg or "permission" in error_msg.lower():
                    print(f"‚ùå CRITICAL: Cannot delete Glue database {db_name} due to permissions: {e}")
                    raise Exception(f"Test setup failed: Cannot delete conflicting Glue database {db_name}. Fix Lake Formation permissions first.")
                else:
                    print(f"‚ö†Ô∏è  Could not delete Glue database {db_name}: {e}")
                    raise

    def teardown_method(self, method):
        """Clean up test environment."""
        super().teardown_method(method)
        self.cleanup_resources()
        self.cleanup_test_directory()

    def cleanup_generated_files(self):
        """Remove generated test files from all target environments AND local code directory."""
        # This method is kept for backward compatibility but no longer generates files
        print("  No generated DAG files to clean (generation disabled)")

    def _cleanup_deployed_generated_files(self):
        """Clean up generated DAG files from deployed S3 locations."""
        # This method is kept for backward compatibility but no longer needed
        pass

    def get_pipeline_file(self):
        """Get path to pipeline file in same directory."""
        return os.path.join(os.path.dirname(__file__), "manifest.yaml")

    @pytest.mark.integration
    def test_describe_connect_after_deploy(self):
        """Test describe --connect after deployment (should be idempotent)."""
        if not self.verify_aws_connectivity():
            pytest.skip("AWS connectivity not available")

        pipeline_file = self.get_pipeline_file()

        # Deploy test target (should be idempotent)
        deploy_result = self.run_cli_command(
            ["deploy", "--manifest", pipeline_file, "--targets", "test"]
        )

        # Deploy should succeed (idempotent behavior)
        assert deploy_result[
            "success"
        ], f"Deploy should be idempotent but failed: {deploy_result['output']}"

        # Test describe --connect after successful deployment
        result = self.run_cli_command(
            ["describe", "--manifest", pipeline_file, "--workflows", "--connect"]
        )

        assert result["success"], f"Describe --connect failed: {result['output']}"

        assert result["success"], f"Describe --connect failed: {result['output']}"

        # Verify it shows pipeline info
        assert "Pipeline: IntegrationTestMultiTarget" in result["output"]
        assert "Domain: cicd-test-domain" in result["output"]  # Region-agnostic

        # Verify it shows target info with connections
        assert "Targets:" in result["output"]
        assert "test: integration-airless-test" in result["output"]
        assert "Project ID:" in result["output"]
        assert "Status:" in result["output"]
        assert "Connections:" in result["output"]

        # Verify Project IDs are not "Unknown"
        assert (
            "Project ID: Unknown" not in result["output"]
        ), "Project ID should not be 'Unknown' when --connect is used"

        # Verify owners information is displayed
        assert (
            "Owners:" in result["output"]
        ), "Owners information should be displayed with --connect"

        # Verify it shows workflow info
        assert "Workflows:" in result["output"]
        assert "test_dag" in result["output"]
        assert "Engine: airflow-serverless" in result["output"]

        # Verify connection details are shown
        assert "connectionId:" in result["output"]
        assert "type:" in result["output"]
        assert "awsAccountId:" in result["output"]

        # Verify MWAA-specific details
        assert "environmentName:" in result["output"]
        # Note: mwaaStatus may not always be present depending on MWAA environment state

    @pytest.mark.integration
    def test_describe_connect_nonexistent_project(self):
        """Test describe --connect with nonexistent project shows proper error."""
        if not self.verify_aws_connectivity():
            pytest.skip("AWS connectivity not available")

        # Create a manifest with a nonexistent project
        manifest_content = """
applicationName: NonexistentProjectTest
content:
stages:
  test:
    stage: test
    domain:
      name: cicd-test-domain
      region: us-east-2
    project:
      name: nonexistent-project-12345
workflows:
  - workflowName: test_dag
    connectionName: project.workflow_mwaa
    logging: console
"""

        # Write temporary manifest
        import tempfile
        import os

        with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
            f.write(manifest_content)
            temp_manifest = f.name

        try:
            result = self.run_cli_command(
                ["describe", "--manifest", temp_manifest, "--connect"]
            )

            # Should succeed but show error for the nonexistent project
            assert result[
                "success"
            ], f"Describe command should not crash: {result['output']}"
            assert (
                "‚ùå Error getting project info:" in result["output"]
            ), f"Should show error for nonexistent project: {result['output']}"
            assert (
                "nonexistent-project-12345" in result["output"]
            ), f"Should mention the project name: {result['output']}"

        finally:
            os.unlink(temp_manifest)

    @pytest.mark.integration
    def test_describe_connect_wrong_domain(self):
        """Test describe --connect with wrong domain shows proper error."""
        if not self.verify_aws_connectivity():
            pytest.skip("AWS connectivity not available")

        # Create a manifest with wrong domain
        manifest_content = """
applicationName: WrongDomainTest
content:
stages:
  test:
    stage: test
    domain:
      name: nonexistent-domain-12345
      region: ${DEV_DOMAIN_REGION:us-east-1}
    project:
      name: integration-test-test
workflows:
  - workflowName: test_dag
    connectionName: project.workflow_mwaa
    logging: console
"""

        # Write temporary manifest
        import tempfile
        import os

        with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
            f.write(manifest_content)
            temp_manifest = f.name

        try:
            result = self.run_cli_command(
                ["describe", "--manifest", temp_manifest, "--connect"]
            )

            # Should succeed - domain validation might not be strict or domain might exist
            assert result[
                "success"
            ], f"Describe command should not crash: {result['output']}"
            # Check that it shows project information (either success or error)
            assert (
                "integration-test-test" in result["output"]
            ), f"Should mention the project name: {result['output']}"

        finally:
            os.unlink(temp_manifest)

    @pytest.mark.integration
    def test_describe_connect_wrong_region(self):
        """Test describe --connect with wrong region shows proper error."""
        if not self.verify_aws_connectivity():
            pytest.skip("AWS connectivity not available")

        # Create a manifest with parameterized region that defaults to wrong region
        manifest_content = """
applicationName: WrongRegionTest
content:
stages:
  test:
    stage: test
    domain:
      name: cicd-test-domain
      region: ${DEV_DOMAIN_REGION:eu-west-1}
    project:
      name: integration-test-test
workflows:
  - workflowName: test_dag
    connectionName: project.workflow_mwaa
    logging: console
"""

        # Write temporary manifest
        import tempfile
        import os

        with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
            f.write(manifest_content)
            temp_manifest = f.name

        try:
            result = self.run_cli_command(
                ["describe", "--manifest", temp_manifest, "--connect"]
            )

            # Should succeed because DEV_DOMAIN_REGION environment variable overrides manifest region
            assert result[
                "success"
            ], f"Describe command should not crash: {result['output']}"
            # Verify that environment variable took precedence and found the project
            assert (
                "Project ID:" in result["output"]
            ), f"Should find project using environment variable region: {result['output']}"

        finally:
            os.unlink(temp_manifest)

    @pytest.mark.integration
    def test_describe_without_connect_vs_with_connect(self):
        """Test difference between describe without --connect and with --connect."""
        if not self.verify_aws_connectivity():
            pytest.skip("AWS connectivity not available")

        pipeline_file = self.get_pipeline_file()

        # Test without --connect
        result_without = self.run_cli_command(["describe", "--manifest", pipeline_file])
        assert result_without[
            "success"
        ], f"Describe without --connect failed: {result_without['output']}"

        # Test with --connect
        result_with = self.run_cli_command(
            ["describe", "--manifest", pipeline_file, "--connect"]
        )
        assert result_with[
            "success"
        ], f"Describe with --connect failed: {result_with['output']}"

        # Without --connect should not have connection details
        assert "Project ID:" not in result_without["output"]
        assert "connectionId:" not in result_without["output"]
        assert "awsAccountId:" not in result_without["output"]
        assert "Owners:" not in result_without["output"]

        # With --connect should have connection details and project info
        assert "Project ID:" in result_with["output"]
        assert "connectionId:" in result_with["output"]
        assert "awsAccountId:" in result_with["output"]
        assert "Owners:" in result_with["output"]

        # Project IDs should not be "Unknown" when using --connect
        assert "Project ID: Unknown" not in result_with["output"]

    @pytest.mark.integration
    def test_describe_connect_project_details(self):
        """Test that describe --connect shows actual Project IDs and Owners information."""
        if not self.verify_aws_connectivity():
            pytest.skip("AWS connectivity not available")

        pipeline_file = self.get_pipeline_file()

        # Test with --connect to get project details
        result = self.run_cli_command(
            ["describe", "--manifest", pipeline_file, "--connect"]
        )
        assert result["success"], f"Describe --connect failed: {result['output']}"

        output = result["output"]

        # Check that all targets show actual Project IDs (not "Unknown")
        assert "Project ID:" in output, "Project ID field should be present"
        assert (
            "Project ID: Unknown" not in output
        ), "Project IDs should not be 'Unknown' with --connect"

        # Check that owners information is displayed
        assert "Owners:" in output, "Owners field should be present with --connect"

        # Check for specific targets and their details
        targets = [
            "dev: dev-marketing",
            "test: integration-test-test",
            "prod: integration-test-prod",
        ]

        for target in targets:
            if target in output:
                # Find the section for this target
                lines = output.split("\n")
                target_found = False
                for i, line in enumerate(lines):
                    if target in line:
                        target_found = True
                        # Check next few lines for project details
                        target_section = "\n".join(lines[i : i + 10])

                        # Verify Project ID is not Unknown
                        if "Project ID:" in target_section:
                            assert (
                                "Project ID: Unknown" not in target_section
                            ), f"Project ID should not be Unknown for {target}"

                        # Verify Owners field exists
                        if "Owners:" in target_section:
                            # Extract owners line and verify it's not empty
                            owners_line = next(
                                (l for l in lines[i : i + 10] if "Owners:" in l), ""
                            )
                            assert (
                                owners_line.strip() != "Owners:"
                            ), f"Owners should not be empty for {target}"

                        break

                if target_found:
                    print(f"‚úÖ {target} - Project details verified")

    @pytest.mark.integration
    def test_describe_connect_multiple_targets_mixed_results(self):
        """Test describe --connect with multiple targets where some exist and some don't."""
        if not self.verify_aws_connectivity():
            pytest.skip("AWS connectivity not available")

        manifest_content = """
applicationName: MixedTargetsTest
content:
stages:
  existing:
    stage: test
    domain:
      name: cicd-test-domain
      region: us-east-2
    project:
      name: integration-test-test
  nonexistent:
    stage: test
    domain:
      name: cicd-test-domain
      region: us-east-2
    project:
      name: definitely-does-not-exist-project-12345
workflows:
  - workflowName: test_dag
    connectionName: project.workflow_mwaa
"""

        # Write temporary manifest
        import tempfile
        import os

        with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
            f.write(manifest_content)
            temp_manifest = f.name

        try:
            result = self.run_cli_command(
                ["describe", "--manifest", temp_manifest, "--connect"]
            )

            # Should succeed and show mixed results
            assert result["success"], f"Command should not crash: {result['output']}"
            assert (
                "Targets:" in result["output"]
            ), f"Should show targets section: {result['output']}"
            assert (
                "existing: integration-test-test" in result["output"]
            ), f"Should show existing target: {result['output']}"
            assert (
                "nonexistent: definitely-does-not-exist-project-12345"
                in result["output"]
            ), f"Should show nonexistent target: {result['output']}"

            # Should show error for nonexistent project
            assert (
                "‚ùå Error getting project info:" in result["output"]
            ), f"Should show error for nonexistent project: {result['output']}"

        finally:
            os.unlink(temp_manifest)

    @pytest.mark.integration
    def test_describe_connect_invalid_connection_name(self):
        """Test describe --connect with workflow referencing nonexistent connection."""
        if not self.verify_aws_connectivity():
            pytest.skip("AWS connectivity not available")

        manifest_content = """
applicationName: InvalidConnectionTest
content:
stages:
  test:
    stage: test
    domain:
      name: cicd-test-domain
      region: us-east-2
    project:
      name: integration-test-test
workflows:
  - workflowName: test_dag
    connectionName: nonexistent.connection.name
"""

        # Write temporary manifest
        import tempfile
        import os

        with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
            f.write(manifest_content)
            temp_manifest = f.name

        try:
            result = self.run_cli_command(
                ["describe", "--manifest", temp_manifest, "--connect"]
            )

            # Should succeed but may show warnings about nonexistent connections
            assert result[
                "success"
            ], f"Command should not crash with invalid connection: {result['output']}"
            assert (
                "Workflows:" in result["output"]
            ), f"Should show workflows section: {result['output']}"
            assert (
                "test_dag" in result["output"]
            ), f"Should show workflow name: {result['output']}"
            assert (
                "nonexistent.connection.name" in result["output"]
            ), f"Should show connection name: {result['output']}"

        finally:
            os.unlink(temp_manifest)

    @pytest.mark.integration
    def test_multi_target_comprehensive_workflow(self):
        """Test complete multi-target pipeline workflow: parse -> upload -> bundle -> deploy -> monitor."""
        if not self.verify_aws_connectivity():
            pytest.skip("AWS connectivity not available")

        pipeline_file = self.get_pipeline_file()
        results = []

        # Step 0: Cleanup - Remove any existing generated files from targets
        print("\n=== Step 0: Cleanup Generated Files ===")
        self.cleanup_generated_files()

        try:
            # Step 1: Describe pipeline configuration
            print("\n=== Step 1: Describe Pipeline ===")
            result = self.run_cli_command(["describe", "--manifest", pipeline_file])
            results.append(result)

            if result["success"]:
                print("‚úÖ Describe command successful")
                assert (
                    "Pipeline:" in result["output"]
                ), f"Describe output missing 'Pipeline:': {result['output']}"
            else:
                print(f"‚ùå Describe command failed: {result['output']}")
                assert False, f"Describe command failed: {result['output']}"

            # Step 2: Describe with targets and connect
            print("\n=== Step 2: Describe with Targets ===")
            result = self.run_cli_command(
                ["describe", "--manifest", pipeline_file, "--connect"]
            )
            results.append(result)

            if result["success"]:
                print("‚úÖ Describe targets successful")
                assert (
                    "Targets:" in result["output"]
                ), f"Describe targets output missing 'Targets:': {result['output']}"
                assert (
                    "Project ID:" in result["output"]
                ), f"Describe targets --connect missing Project ID info: {result['output']}"

                # Validate that Eng1 (c478f4d8-4061-7042-f852-70c2db7c217e) is an owner in test and prod projects
                if "c478f4d8-4061-7042-f852-70c2db7c217e" in result["output"]:
                    print("‚úÖ Eng1 is correctly listed as project owner")
                else:
                    print(
                        "‚ö†Ô∏è  Eng1 not found as project owner - may need initialization"
                    )
            else:
                print(f"‚ùå Describe targets failed: {result['output']}")
                assert False, f"Describe targets failed: {result['output']}"

        except Exception as e:
            print(f"‚ùå Describe steps failed: {e}")
            results.append({"success": False, "output": f"Describe steps failed: {e}"})

        # Step 3: Upload code files to S3 (test S3 connection)
        print("\n=== Step 3: Upload Code to S3 ===")
        try:
            # Get S3 URI from dev project using describe command
            describe_result = self.run_cli_command(
                ["describe", "--manifest", pipeline_file, "--connect"]
            )
            if describe_result["success"]:
                describe_output = describe_result["output"]

                # Extract S3 URI for dev project from describe output
                import re

                s3_uri_match = re.search(
                    r"dev: dev-marketing.*?s3Uri: (s3://[^\s]+)",
                    describe_output,
                    re.DOTALL,
                )

                if s3_uri_match:
                    s3_uri = s3_uri_match.group(1)

                    # Copy local code files to S3
                    code_dir = os.path.join(os.path.dirname(pipeline_file), "code")
                    if os.path.exists(code_dir):
                        import subprocess

                        result = subprocess.run(
                            [
                                "aws",
                                "s3",
                                "sync",
                                code_dir,
                                s3_uri,
                                "--exclude",
                                "*.pyc",
                                "--exclude",
                                "__pycache__/*",
                            ],
                            capture_output=True,
                            text=True,
                        )

                        if result.returncode == 0:
                            print(f"‚úÖ Code uploaded to S3: {s3_uri}")
                            print(f"Upload output: {result.stdout.strip()}")
                            results.append(
                                {
                                    "success": True,
                                    "output": f"Code uploaded to {s3_uri}",
                                }
                            )
                        else:
                            print(f"‚ùå S3 upload failed: {result.stderr}")
                            results.append(
                                {
                                    "success": False,
                                    "output": f"S3 upload failed: {result.stderr}",
                                }
                            )
                    else:
                        print(f"‚ö†Ô∏è  Code directory not found: {code_dir}")
                        results.append(
                            {
                                "success": False,
                                "output": f"Code directory not found: {code_dir}",
                            }
                        )
                else:
                    print("‚ùå S3 URI not found in describe output")
                    results.append(
                        {
                            "success": False,
                            "output": "S3 URI not found in describe output",
                        }
                    )
            else:
                print(f"‚ùå Failed to get project info: {describe_result['output']}")
                results.append(
                    {
                        "success": False,
                        "output": f"Failed to get project info: {describe_result['output']}",
                    }
                )
        except Exception as e:
            print(f"‚ùå S3 upload error: {e}")
            results.append({"success": False, "output": f"S3 upload error: {e}"})

        # Step 4: Bundle command for dev target
        print("\n=== Step 4: Bundle Command (dev) ===")
        result = self.run_cli_command(["bundle", "--manifest", pipeline_file, "dev"])
        results.append(result)

        if result["success"]:
            print("‚úÖ Bundle command successful")
            print(f"Bundle output: {result['output']}")

            # Extract bundle file path from output
            import re

            bundle_match = re.search(r"Bundle created: (.*\.zip)", result["output"])
            if bundle_match:
                bundle_path = bundle_match.group(1).strip()
                print(f"Bundle file location: {bundle_path}")

                # Check if bundle file exists and is not empty
                if os.path.exists(bundle_path):
                    file_size = os.path.getsize(bundle_path)
                    print(f"Bundle file size: {file_size} bytes")
                    assert file_size > 0, f"Bundle file is empty: {bundle_path}"

                    # Check bundle contents for uploaded files
                    import zipfile

                    with zipfile.ZipFile(bundle_path, "r") as zip_file:
                        file_list = zip_file.namelist()
                        print(f"Bundle contains {len(file_list)} files")

                        # Check for uploaded files in their respective directories
                        assert any(
                            "workflows/dags/test_dag.py" in f for f in file_list
                        ), f"workflows/dags/test_dag.py not found in bundle: {file_list}"
                        assert any(
                            "code/test-notebook1.ipynb" in f for f in file_list
                        ), f"code/test-notebook1.ipynb not found in bundle: {file_list}"

                        print(
                            "‚úÖ Bundle contains uploaded files: workflows/dags/test_dag.py and code/test-notebook1.ipynb"
                        )

                    print("‚úÖ Bundle file exists and is not empty")
                else:
                    assert False, f"Bundle file not found: {bundle_path}"
            else:
                # Fallback validation
                assert (
                    "Bundle created" in result["output"]
                    or "bundling" in result["output"].lower()
                ), f"Bundle output missing success indicator: {result['output']}"
        else:
            # Bundle should succeed - any failure is a real test failure
            pytest.fail(f"Bundle command failed: {result['output']}")

        # Step 5: Deploy to test target (auto-initializes if needed)
        print("\n=== Step 5: Deploy to Test Target ===")
        result = self.run_cli_command(
            ["deploy", "--manifest", pipeline_file, "--targets", "test"]
        )
        results.append(result)

        # Deploy should be idempotent and succeed
        assert result[
            "success"
        ], f"Deploy should be idempotent but failed: {result['output']}"
        print("‚úÖ Deploy command successful (idempotent)")
        print(f"Deploy output: {result['output']}")

        # Validate Airflow Serverless workflow creation
        self._validate_airflow_serverless_workflows(result['output'])

        # Step 6: Deploy command (test target)
        print("\n=== Step 6: Deploy Command (test) ===")
        result = self.run_cli_command(["deploy", "--manifest", pipeline_file, "test"])
        results.append(result)

        if result["success"]:
            print("‚úÖ Deploy command successful")
            print(f"Deploy output: {result['output']}")

            # Validate workflow validation process
            deploy_output = result["output"]
            assert (
                "üöÄ Starting workflow validation..." in deploy_output
            ), f"Deploy output missing workflow validation start: {deploy_output}"

            # Deploy should just complete workflow validation without MWAA checks
            assert (
                "‚úÖ Workflow validation completed" in deploy_output
            ), f"Deploy output missing workflow validation completion: {deploy_output}"
            print("‚úÖ Workflow validation completed successfully")

            # Validate S3 destination structure after deploy
            try:
                import subprocess
                import re

                # Extract actual S3 location from deploy output
                s3_location_match = re.search(
                    r"S3 Location: (s3://[^\s]+)", deploy_output
                )
                if s3_location_match:
                    s3_location = s3_location_match.group(1)
                    print(f"‚úÖ Found S3 location: {s3_location}")

                    s3_result = subprocess.run(
                        ["aws", "s3", "ls", s3_location, "--recursive"],
                        capture_output=True,
                        text=True,
                    )

                    if s3_result.returncode == 0:
                        s3_files = s3_result.stdout.strip().split("\n")
                        deployed_files = [
                            line.split()[-1] for line in s3_files if line.strip()
                        ]

                        # Check for expected deployed files (using relative paths)
                        has_notebook = any(
                            "test-notebook1.ipynb" in f for f in deployed_files
                        )
                        has_dag = any("test_dag.py" in f for f in deployed_files)

                        if has_notebook and has_dag:
                            print("‚úÖ S3 validation successful - found expected files")
                        else:
                            print(
                                f"‚ö†Ô∏è  S3 validation incomplete - files: {deployed_files}"
                            )
                    else:
                        print(f"‚ö†Ô∏è  Could not list S3 contents: {s3_result.stderr}")
                else:
                    print("‚ö†Ô∏è  Could not extract S3 location from deploy output")

            except Exception as e:
                print(f"‚ö†Ô∏è  S3 validation error: {e}")
        else:
            # Deploy should be idempotent and succeed
            assert (
                False
            ), f"Deploy to test should be idempotent but failed: {result['output']}"

        # Step 7: Deploy command (prod target)
        print("\n=== Step 7: Deploy Command (prod) ===")
        result = self.run_cli_command(["deploy", "--manifest", pipeline_file, "prod"])
        results.append(result)

        if result["success"]:
            print("‚úÖ Deploy to prod successful")
            print(f"Deploy output: {result['output']}")

            # Validate deployment completed
            assert (
                "‚úÖ Deployment completed successfully!" in result["output"]
            ), f"Deploy output missing completion: {result['output']}"
            assert (
                "üìä Total files deployed:" in result["output"]
            ), f"Deploy output missing deploy count: {result['output']}"
        else:
            # Deploy should be idempotent and succeed
            assert (
                False
            ), f"Deploy to prod should be idempotent but failed: {result['output']}"

        # Step 8: Describe with Connect (after deployment)
        print("\n=== Step 8: Describe with Connect ===")
        result = self.run_cli_command(
            ["describe", "--manifest", pipeline_file, "--connect"]
        )
        results.append(result)

        if result["success"]:
            print("‚úÖ Describe --connect successful")

            # Validate it shows connection details
            describe_output = result["output"]
            assert (
                "Pipeline: IntegrationTestMultiTarget" in describe_output
            ), f"Describe output missing pipeline name: {describe_output}"
            # Get the actual region from environment variable or default
            expected_region = os.environ.get('DEV_DOMAIN_REGION', 'us-east-2')
            assert (
                f"Domain: cicd-test-domain ({expected_region})"
                in describe_output
            ), f"Describe output missing domain info: {describe_output}"
            assert (
                "Targets:" in describe_output
            ), f"Describe output missing targets section: {describe_output}"

            # Check for connection details (if project exists)
            if "Error:" not in describe_output:
                assert (
                    "Project ID:" in describe_output
                ), f"Describe --connect missing project ID: {describe_output}"
                assert (
                    "Connections:" in describe_output
                ), f"Describe --connect missing connections: {describe_output}"
                assert (
                    "connectionId:" in describe_output
                ), f"Describe --connect missing connection details: {describe_output}"
                print("‚úÖ Connection details validated")
            else:
                print("‚ö†Ô∏è  Project not found - connection details not available")
        else:
            print(f"‚ùå Describe --connect failed: {result['output']}")

        # Step 8.5: Run command to trigger the workflow (DISABLED - service issue with deleting running workflows)
        print("\n=== Step 8.5: Run Command (DISABLED) ===")
        print("‚ö†Ô∏è Run command disabled due to service issue with deleting running workflows")
        # run_result = self.run_cli_command(
        #     [
        #         "run",
        #         "--manifest",
        #         pipeline_file,
        #         "--workflow",
        #         "test_dag_v2",  # Changed DAG name
        #         "--command",
        #         "dags trigger test_dag_v2",
        #         "--targets",
        #         "test",
        #     ]
        # )
        # results.append(run_result)
        
        # Create a mock result for consistency
        run_result = {"success": True, "output": "Run command disabled", "command": "run (disabled)"}
        results.append(run_result)

        # Run command should succeed (mocked as successful)
        print("‚úÖ Run command disabled (mocked as successful)")
        print("Run output: Run command disabled")

        # Step 8.6: Monitor command to check actual DAG detection
        print("\n=== Step 8.6: Monitor Command (DAG Detection) ===")
        result = self.run_cli_command(["monitor", "--manifest", pipeline_file])
        results.append(result)

        if result["success"]:
            print("‚úÖ Monitor command successful")
            monitor_output = result["output"]

            # Check if the actual deployed workflow is detected in at least one environment
            expected_workflow = "IntegrationTestMultiTargetAirless_integration_airless_test_test_dag"
            if expected_workflow in monitor_output:
                print(f"‚úÖ Workflow '{expected_workflow}' detected in Airflow Serverless")

                # Count how many targets show the workflow
                workflow_count = monitor_output.count(expected_workflow)
                print(f"‚úÖ Workflow detected in {workflow_count} target environment(s)")
            else:
                print(f"‚ùå Workflow '{expected_workflow}' not detected in any environments")
                print("üîç Monitor output for debugging:")
                print(monitor_output)
                # This is a critical failure - at least test environment should show the DAG
                assert (
                    False
                ), f"Expected workflow '{expected_workflow}' not found in monitor output after successful deployment: {monitor_output}"

            print(f"Monitor output: {result['output']}")
        else:
            print(f"‚ùå Monitor command failed: {result['output']}")

            # Check if failure is due to MWAA unavailability (expected in test environment)
            if (
                "MWAA environment connection not found" in result["output"]
                and "No healthy MWAA environments found" in result["output"]
            ):
                print(
                    "‚ö†Ô∏è Monitor failed due to MWAA unavailability - this is expected in test environment"
                )
                print(
                    "‚úÖ All deployment steps completed successfully, MWAA monitoring skipped"
                )
            else:
                assert (
                    False
                ), f"Monitor command failed for unexpected reason: {result['output']}"

        print(f"\n‚úÖ All CLI commands tested successfully!")

        # Generate test report
        report = self.generate_test_report(
            "Multi-Target Pipeline Comprehensive Workflow", results
        )

        print(f"\n=== Test Report ===")
        print(f"Test: {report['test_name']}")
        print(f"Commands executed: {report['total_commands']}")
        print(f"Successful commands: {report['successful_commands']}")
        print(f"Success rate: {report['success_rate']:.1%}")
        print(f"Overall success: {'‚úÖ' if report['overall_success'] else '‚ùå'}")

        # Parse, S3 upload, bundle commands must succeed
        # Deploy commands should also succeed (idempotent behavior)
        critical_commands = results[
            :4
        ]  # First 4 are critical (parse, parse targets, S3 upload, bundle)
        critical_success = all(r["success"] for r in critical_commands)

        # All deploy commands should succeed due to idempotent behavior
        deploy_commands = [
            r for r in results if r.get("command") and "deploy" in r["command"]
        ]
        deploy_success = all(r["success"] for r in deploy_commands)

        # Run command is expected to fail due to workflow name mismatch - exclude from critical commands
        critical_commands_without_run = [r for r in critical_commands if not (r.get("command") and "run" in r["command"])]
        critical_success_without_run = all(r["success"] for r in critical_commands_without_run)

        assert (
            critical_success_without_run
        ), f"Critical commands (excluding run) must succeed: {[r for r in critical_commands_without_run if not r['success']]}"
        assert (
            deploy_success
        ), f"Deploy commands should be idempotent and succeed: {[r for r in deploy_commands if not r['success']]}"
        assert (
            len(results) == 10
        ), f"Expected 10 commands (2 describe + 1 S3 upload + 1 bundle + 3 deploy + 1 describe + 1 run + 1 monitor), got {len(results)}"

        # Cleanup after successful test completion
        print(f"\n=== Final Cleanup ===")
        self.cleanup_generated_files()
        print(f"‚úÖ Test completed successfully and cleaned up generated files")

    def test_catalog_asset_access_workflow(self):
        """Test catalog asset access functionality during deployment."""
        # TODO: Re-enable catalog validation tests after workflow name issues are resolved
        pytest.skip("Catalog validation tests temporarily disabled")
        
        if not self.verify_aws_connectivity():
            pytest.skip("AWS connectivity not available")

        pipeline_file = self.get_pipeline_file()
        results = []

        print("\n=== Catalog Asset Access Test ===")
        
        try:
            # Step 1: Verify manifest has catalog assets configured
            print("\n--- Step 1: Verify Catalog Configuration ---")
            from smus_cicd.application import ApplicationManifest
            
            manifest = ApplicationManifest.from_file(pipeline_file)
            
            if not manifest.content.catalog or not manifest.content.catalog.assets:
                pytest.skip("No catalog assets configured in manifest")
            
            print(f"‚úÖ Found {len(manifest.content.catalog.assets)} catalog assets")
            for i, asset in enumerate(manifest.content.catalog.assets):
                identifier = asset.selector.search.identifier if asset.selector.search else asset.selector.assetId
                print(f"   Asset {i+1}: {identifier} (permission: {asset.permission})")
            
            # Step 2: Cancel any existing subscriptions to test fresh workflow
            print("\n--- Step 2: Cancel Existing Subscriptions ---")
            try:
                from smus_cicd.helpers.datazone import get_domain_id_by_name, get_project_id_by_name
                import boto3
                
                target_config = manifest.get_stage('test')
                domain_name = target_config.domain.name
                project_name = target_config.project.name
                region = target_config.domain.region
                
                domain_id = get_domain_id_by_name(domain_name, region)
                project_id = get_project_id_by_name(project_name, domain_id, region)
                
                if domain_id and project_id:
                    datazone_client = boto3.client('datazone', region_name=region)
                    
                    # Cancel existing subscriptions for test assets
                    for asset in manifest.content.catalog.assets:
                        if asset.selector.search:
                            identifier = asset.selector.search.identifier
                            print(f"  Checking subscriptions for: {identifier}")
                            
                            # Search for the asset to get listing ID
                            search_response = datazone_client.search_listings(
                                domainIdentifier=domain_id,
                                searchText=identifier
                            )
                            
                            if search_response.get('items'):
                                listing_id = search_response['items'][0]['assetListing']['listingId']
                                
                                # Check for active subscriptions
                                subs_response = datazone_client.list_subscriptions(
                                    domainIdentifier=domain_id,
                                    owningProjectId=project_id
                                )
                                
                                for sub in subs_response.get('items', []):
                                    if sub.get('subscribedListing', {}).get('id') == listing_id:
                                        sub_id = sub['id']
                                        print(f"  Canceling existing subscription: {sub_id}")
                                        
                                        try:
                                            datazone_client.cancel_subscription(
                                                domainIdentifier=domain_id,
                                                identifier=sub_id
                                            )
                                            print(f"  ‚úÖ Canceled subscription: {sub_id}")
                                        except Exception as e:
                                            print(f"  ‚ö†Ô∏è Could not cancel subscription {sub_id}: {e}")
                                            
                print("‚úÖ Subscription cleanup completed")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Subscription cleanup error: {e}")
                # Continue with test - cleanup is best effort
            
            # Step 3: First deploy - should create new subscriptions
            print("\n--- Step 3: First Deploy (Create Subscriptions) ---")
            result = self.run_cli_command(
                ["deploy", "--manifest", pipeline_file, "--targets", "test"]
            )
            results.append(result)
            
            if result["success"]:
                print("‚úÖ First deploy successful")
                deploy_output = result["output"]
                
                # Should show subscription creation
                if "Created subscription request" in deploy_output:
                    print("‚úÖ New subscription request created as expected")
                elif "Using existing subscription" in deploy_output:
                    print("‚ÑπÔ∏è Found existing subscription (cleanup may not have completed)")
                
                print(f"First deploy output: {deploy_output}")
                
            else:
                pytest.fail(f"First deploy failed: {result['output']}")
            
            # Step 4: Second deploy - should be idempotent
            print("\n--- Step 4: Second Deploy (Idempotency Test) ---")
            result = self.run_cli_command(
                ["deploy", "--manifest", pipeline_file, "--targets", "test"]
            )
            results.append(result)
            
            if result["success"]:
                print("‚úÖ Second deploy successful (idempotent)")
                deploy_output = result["output"]
                
                # Should use existing subscription
                if "Using existing subscription" in deploy_output:
                    print("‚úÖ Correctly used existing subscription (idempotent)")
                elif "Created subscription request" in deploy_output:
                    print("‚ö†Ô∏è Created new subscription on second deploy - may not be fully idempotent")
                
                # Should still process catalog assets
                assert (
                    "Processing catalog assets" in deploy_output or
                    "catalog assets" in deploy_output.lower()
                ), f"Second deploy missing catalog asset processing: {deploy_output}"
                
                print(f"Second deploy output: {deploy_output}")
                
            else:
                pytest.fail(f"Second deploy failed: {result['output']}")
            
            # Step 5: Verify DataZone integration
            print("\n--- Step 5: Verify DataZone Integration ---")
            try:
                from smus_cicd.helpers.datazone import search_asset_listing
                
                print(f"Testing DataZone connectivity:")
                print(f"  Domain: {domain_name}")
                print(f"  Project: {project_name}")
                print(f"  Region: {region}")
                
                if domain_id:
                    print(f"‚úÖ Domain ID resolved: {domain_id}")
                    
                    if project_id:
                        print(f"‚úÖ Project ID resolved: {project_id}")
                        
                        # Test asset search functionality
                        for asset in manifest.content.catalog.assets:
                            if asset.selector.search:
                                identifier = asset.selector.search.identifier
                                print(f"  Testing asset search: {identifier}")
                                
                                result = search_asset_listing(domain_id, identifier, region)
                                if result:
                                    asset_id, listing_id = result
                                    print(f"‚úÖ Asset found: {asset_id}, listing: {listing_id}")
                                else:
                                    print(f"‚ö†Ô∏è Asset not found: {identifier}")
                    else:
                        print(f"‚ö†Ô∏è Project ID not resolved for: {project_name}")
                else:
                    print(f"‚ö†Ô∏è Domain ID not resolved for: {domain_name}")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è DataZone integration test error: {e}")
            
            # Step 6: Validate overall success
            print("\n--- Step 6: Validate Results ---")
            
            # Both deploys should succeed
            deploy_successes = [r.get("success", False) for r in results]
            
            if all(deploy_successes):
                print("‚úÖ Catalog asset access test completed successfully")
                print("   - First deploy: Created/found asset subscriptions")
                print("   - Second deploy: Idempotent behavior confirmed")
                print("   - DataZone connectivity validated")
                print("   - Complete workflow tested")
            else:
                failed_deploys = [i for i, success in enumerate(deploy_successes) if not success]
                pytest.fail(f"Deploy(s) failed: {failed_deploys}")
                
        except Exception as e:
            print(f"‚ùå Catalog asset access test failed: {e}")
            pytest.fail(f"Catalog asset access test failed: {e}")

    def test_catalog_asset_backward_compatibility(self):
        """Test that catalog assets don't break existing functionality when not configured."""
        # TODO: Re-enable catalog validation tests after workflow name issues are resolved
        pytest.skip("Catalog validation tests temporarily disabled")
        
        if not self.verify_aws_connectivity():
            pytest.skip("AWS connectivity not available")

        print("\n=== Catalog Asset Backward Compatibility Test ===")
        
        try:
            # Create a temporary manifest without catalog section
            import tempfile
            import yaml
            
            pipeline_file = self.get_pipeline_file()
            
            # Load existing manifest
            with open(pipeline_file, 'r') as f:
                manifest_data = yaml.safe_load(f)
            
            # Remove catalog section if it exists
            if 'catalog' in manifest_data.get('bundle', {}):
                del manifest_data['bundle']['catalog']
                print("‚úÖ Removed catalog section from test manifest")
            
            # Write temporary manifest
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                yaml.dump(manifest_data, f)
                temp_manifest = f.name
            
            try:
                # Test deploy without catalog assets
                print("\n--- Testing Deploy Without Catalog Assets ---")
                result = self.run_cli_command(
                    ["deploy", "--manifest", temp_manifest, "--targets", "test"]
                )
                
                if result["success"]:
                    print("‚úÖ Deploy without catalog assets successful")
                    
                    # Should not contain catalog processing messages
                    deploy_output = result["output"]
                    if "catalog assets" in deploy_output.lower():
                        # Should say "No catalog assets" not processing them
                        assert (
                            "No catalog assets" in deploy_output or
                            "catalog assets to process" in deploy_output
                        ), f"Unexpected catalog processing in output: {deploy_output}"
                        print("‚úÖ Correctly handled missing catalog configuration")
                    else:
                        print("‚úÖ No catalog processing (as expected)")
                        
                else:
                    pytest.fail(f"Deploy without catalog assets failed: {result['output']}")
                    
            finally:
                # Clean up temporary file
                import os
                os.unlink(temp_manifest)
                
            print("‚úÖ Backward compatibility test passed")
            
        except Exception as e:
            print(f"‚ùå Backward compatibility test failed: {e}")
            pytest.fail(f"Backward compatibility test failed: {e}")
    def test_catalog_asset_negative_scenarios(self):
        """Test catalog asset negative scenarios and error handling."""
        # TODO: Re-enable catalog validation tests after workflow name issues are resolved
        pytest.skip("Catalog validation tests temporarily disabled")
        
        if not self.verify_aws_connectivity():
            pytest.skip("AWS connectivity not available")

        print("\n=== Catalog Asset Negative Scenarios Test ===")
        
        # Test 1: Invalid asset identifier
        print("\n--- Test 1: Invalid Asset Identifier ---")
        try:
            import tempfile
            import yaml
            
            pipeline_file = self.get_pipeline_file()
            
            # Load existing manifest and modify it
            with open(pipeline_file, 'r') as f:
                manifest_data = yaml.safe_load(f)
            
            # Add catalog with invalid asset
            manifest_data['bundle']['catalog'] = {
                'assets': [{
                    'selector': {
                        'search': {
                            'assetType': 'GlueTable',
                            'identifier': 'nonexistent_db.nonexistent_table'
                        }
                    },
                    'permission': 'READ',
                    'requestReason': 'Test invalid asset'
                }]
            }
            
            # Write temporary manifest
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                yaml.dump(manifest_data, f)
                temp_manifest = f.name
            
            try:
                # Deploy should fail gracefully
                result = self.run_cli_command(
                    ["deploy", "--manifest", temp_manifest, "--targets", "test"]
                )
                
                # Should fail but with proper error message
                assert result["success"] == False
                print("‚úÖ Invalid asset identifier handled correctly")
                
            finally:
                import os
                os.unlink(temp_manifest)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Test 1 error: {e}")
        
        # Test 2: Malformed catalog configuration
        print("\n--- Test 2: Malformed Catalog Configuration ---")
        try:
            # Create manifest with missing required fields
            manifest_data = yaml.safe_load(open(pipeline_file, 'r').read())
            manifest_data['bundle']['catalog'] = {
                'assets': [{
                    'selector': {
                        # Missing search or assetId
                    },
                    'permission': 'READ',
                    'requestReason': 'Test malformed config'
                }]
            }
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                yaml.dump(manifest_data, f)
                temp_manifest = f.name
            
            try:
                # Should fail during manifest parsing
                from smus_cicd.application import ApplicationManifest
                
                try:
                    manifest = ApplicationManifest.from_file(temp_manifest)
                    print("‚ö†Ô∏è Malformed config was accepted (unexpected)")
                except Exception as e:
                    print(f"‚úÖ Malformed config rejected: {type(e).__name__}")
                    
            finally:
                os.unlink(temp_manifest)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Test 2 error: {e}")
        
        # Test 3: Invalid permission values
        print("\n--- Test 3: Invalid Permission Values ---")
        try:
            manifest_data = yaml.safe_load(open(pipeline_file, 'r').read())
            manifest_data['bundle']['catalog'] = {
                'assets': [{
                    'selector': {
                        'search': {
                            'assetType': 'GlueTable',
                            'identifier': 'covid19_db.countries_aggregated'
                        }
                    },
                    'permission': 'INVALID_PERMISSION',  # Invalid permission
                    'requestReason': 'Test invalid permission'
                }]
            }
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                yaml.dump(manifest_data, f)
                temp_manifest = f.name
            
            try:
                # Should be handled by schema validation
                from smus_cicd.application import ApplicationManifest
                
                try:
                    manifest = ApplicationManifest.from_file(temp_manifest)
                    print("‚ö†Ô∏è Invalid permission was accepted (check schema)")
                except Exception as e:
                    print(f"‚úÖ Invalid permission rejected: {type(e).__name__}")
                    
            finally:
                os.unlink(temp_manifest)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Test 3 error: {e}")
        
        # Test 4: Empty assets array
        print("\n--- Test 4: Empty Assets Array ---")
        try:
            manifest_data = yaml.safe_load(open(pipeline_file, 'r').read())
            manifest_data['bundle']['catalog'] = {
                'assets': []  # Empty array
            }
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                yaml.dump(manifest_data, f)
                temp_manifest = f.name
            
            try:
                # Should succeed but skip catalog processing
                result = self.run_cli_command(
                    ["deploy", "--manifest", temp_manifest, "--targets", "test"]
                )
                
                if result["success"]:
                    print("‚úÖ Empty assets array handled correctly")
                else:
                    print("‚ö†Ô∏è Empty assets array caused failure")
                    
            finally:
                os.unlink(temp_manifest)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Test 4 error: {e}")
        
        print("\n‚úÖ Negative scenarios testing completed")

    def test_catalog_asset_mixed_scenarios(self):
        """Test mixed valid/invalid catalog assets."""
        # TODO: Re-enable catalog validation tests after workflow name issues are resolved
        pytest.skip("Catalog validation tests temporarily disabled")
        
        if not self.verify_aws_connectivity():
            pytest.skip("AWS connectivity not available")

        print("\n=== Mixed Catalog Asset Scenarios Test ===")
        
        try:
            import tempfile
            import yaml
            
            pipeline_file = self.get_pipeline_file()
            
            # Create manifest with mixed valid/invalid assets
            with open(pipeline_file, 'r') as f:
                manifest_data = yaml.safe_load(f)
            
            manifest_data['bundle']['catalog'] = {
                'assets': [
                    {
                        'selector': {
                            'search': {
                                'assetType': 'GlueTable',
                                'identifier': 'covid19_db.countries_aggregated'  # Valid
                            }
                        },
                        'permission': 'READ',
                        'requestReason': 'Valid asset for testing'
                    },
                    {
                        'selector': {
                            'search': {
                                'assetType': 'GlueTable',
                                'identifier': 'nonexistent_db.invalid_table'  # Invalid
                            }
                        },
                        'permission': 'READ',
                        'requestReason': 'Invalid asset for testing'
                    }
                ]
            }
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                yaml.dump(manifest_data, f)
                temp_manifest = f.name
            
            try:
                # Deploy should fail on first invalid asset
                result = self.run_cli_command(
                    ["deploy", "--manifest", temp_manifest, "--targets", "test"]
                )
                
                # Should fail because of invalid asset
                if not result["success"]:
                    print("‚úÖ Mixed assets correctly failed on invalid asset")
                else:
                    print("‚ö†Ô∏è Mixed assets unexpectedly succeeded")
                    
            finally:
                import os
                os.unlink(temp_manifest)
                
        except Exception as e:
            print(f"‚ùå Mixed scenarios test failed: {e}")
            
        print("‚úÖ Mixed scenarios testing completed")

    def _validate_airflow_serverless_workflows(self, deploy_output):
        """Validate Airflow Serverless workflow creation in deploy output."""
        # Check for Airflow Serverless workflow creation
        if "‚úÖ Created Overdrive workflow:" in deploy_output:
            print("‚úÖ Airflow Serverless workflow created successfully")
            
            # Validate ARN is displayed
            if "ARN: arn:aws:airflow-serverless:" in deploy_output:
                print("‚úÖ Real Airflow Serverless ARN displayed")
            else:
                print("‚ö†Ô∏è Airflow Serverless ARN not found in output")
                
            # Validate status is READY
            if "Status: READY" in deploy_output:
                print("‚úÖ Workflow status is READY")
            else:
                print("‚ö†Ô∏è Workflow status not READY")
        else:
            print("‚ö†Ô∏è Airflow Serverless workflow creation not detected")
