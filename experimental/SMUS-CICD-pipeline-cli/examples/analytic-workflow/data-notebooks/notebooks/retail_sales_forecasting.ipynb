{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail Sales Forecasting with SageMaker XGBoost\n",
    "\n",
    "This notebook demonstrates end-to-end retail sales forecasting using Amazon SageMaker's managed XGBoost container. We'll cover:\n",
    "\n",
    "1. **Data preprocessing** - Load and prepare retail sales data\n",
    "2. **Feature engineering** - Create time-series features for forecasting\n",
    "3. **Model training** - Train XGBoost model using SageMaker\n",
    "4. **Hyperparameter tuning** - Optimize model performance automatically\n",
    "5. **Batch inference** - Generate predictions on test data\n",
    "6. **Resource cleanup** - Clean up resources\n",
    "\n",
    "## Dataset Attribution\n",
    "Â© Chen, D. (2012). Online Retail II [Dataset]. UCI Machine Learning Repository. Available at: https://archive.ics.uci.edu/dataset/502/online+retail+ii. Licensed under Creative Commons Attribution 4.0 International (CC BY 4.0) license, which can be found here: https://creativecommons.org/licenses/by/4.0/legalcode#s6a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q sagemaker pandas numpy matplotlib seaborn scikit-learn xgboost boto3 joblib\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'retail-sales-forecasting'\n",
    "\n",
    "# Create unique session ID for resource tracking\n",
    "session_id = f\"{int(time.time())}\"\n",
    "print(f\"Session ID: {session_id} (for resource cleanup)\")\n",
    "\n",
    "print(f\"Role: {role}\")\n",
    "print(f\"Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "current_region = boto3.Session().region_name or \"us-west-2\"\n",
    "data_url = f\"s3://sagemaker-example-files-prod-{current_region}/datasets/tabular/online_retail/online_retail_II_20k.csv\"\n",
    "df = pd.read_csv(data_url)\n",
    "df = df.dropna(subset=[\"Customer ID\"])\n",
    "df = df[(df[\"Quantity\"] > 0) & (df[\"Price\"] > 0)]\n",
    "df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"])\n",
    "df[\"Revenue\"] = df[\"Quantity\"] * df[\"Price\"]\n",
    "\n",
    "# Aggregate daily sales\n",
    "daily_sales = df.groupby(df['InvoiceDate'].dt.date).agg({\n",
    "    'Revenue': 'sum', 'Quantity': 'sum', 'Invoice': 'nunique', 'Customer ID': 'nunique'\n",
    "}).reset_index()\n",
    "daily_sales.columns = ['Date', 'Revenue', 'Quantity', 'Orders', 'Customers']\n",
    "daily_sales['Date'] = pd.to_datetime(daily_sales['Date'])\n",
    "daily_sales = daily_sales.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "print(f'Summary of Revenue')\n",
    "print(daily_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Quarter'] = df['Date'].dt.quarter\n",
    "    df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
    "    \n",
    "    # Reduced lag features to preserve data\n",
    "    for lag in [1, 2]:\n",
    "        df[f'Revenue_lag_{lag}'] = df['Revenue'].shift(lag)\n",
    "    \n",
    "    # Smaller rolling window\n",
    "    df['Revenue_ma_3'] = df['Revenue'].rolling(window=3).mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "daily_sales_features = create_features(daily_sales)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "daily_sales_features = daily_sales_features.dropna().reset_index(drop=True)\n",
    "\n",
    "if len(daily_sales_features) < 5:\n",
    "    raise ValueError(f\"Insufficient data after feature engineering: {len(daily_sales_features)} rows\")\n",
    "\n",
    "print(f\"Final dataset has {len(daily_sales_features)} rows for modeling\")\n",
    "print(daily_sales_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "From the Final Datasets that has 6 rows, first 4 are used for training and 2 will be used as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "feature_cols = [col for col in daily_sales_features.columns if col not in ['Date', 'Revenue']]\n",
    "print(f\"Feature columns: {feature_cols}\")\n",
    "\n",
    "split_idx = int(len(daily_sales_features) * 0.8)\n",
    "train_data = daily_sales_features[:split_idx]\n",
    "test_data = daily_sales_features[split_idx:]\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}, Test data shape: {test_data.shape}\")\n",
    "\n",
    "# XGBoost format (target first)\n",
    "train_xgb = pd.concat([train_data['Revenue'], train_data[feature_cols]], axis=1)\n",
    "test_xgb = pd.concat([test_data['Revenue'], test_data[feature_cols]], axis=1)\n",
    "\n",
    "print(f\"Train data shape before saving: {train_xgb.shape}\")\n",
    "print(f\"Test data shape before saving: {test_xgb.shape}\")\n",
    "print(f\"Train data sample:\\n{train_xgb.head()}\")\n",
    "\n",
    "os.makedirs(\"notebook_outputs\", exist_ok=True)\n",
    "\n",
    "# Upload to S3\n",
    "try:\n",
    "    train_xgb.to_csv('notebook_outputs/train.csv', index=False, header=False)\n",
    "    test_xgb.to_csv('notebook_outputs/test.csv', index=False, header=False)\n",
    "    \n",
    "    # Verify files were created and have content\n",
    "    train_size = os.path.getsize('notebook_outputs/train.csv')\n",
    "    test_size = os.path.getsize('notebook_outputs/test.csv')\n",
    "    print(f\"Local train.csv size: {train_size} bytes\")\n",
    "    print(f\"Local test.csv size: {test_size} bytes\")\n",
    "    \n",
    "    if train_size == 0:\n",
    "        raise ValueError(\"train.csv is empty!\")\n",
    "    \n",
    "    train_path = sagemaker_session.upload_data('notebook_outputs/train.csv', bucket, f'{prefix}/data')\n",
    "    test_path = sagemaker_session.upload_data('notebook_outputs/test.csv', bucket, f'{prefix}/data')\n",
    "    \n",
    "    print(f\"Training data: {len(train_data)} days, Test data: {len(test_data)} days\")\n",
    "    print(f\"Train path: {train_path}\")\n",
    "    print(f\"Test path: {test_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in data preparation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training with Managed XGBoost Container\n",
    "\n",
    "## You can switch the instance type to GPU instance from ml.m5.large to reduce the training time (5 minutes), however selecting the GPU instance will result in cost increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SageMaker's built-in XGBoost algorithm\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "container = get_image_uri(region, 'xgboost', repo_version='1.7-1')\n",
    "\n",
    "xgb_estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    objective='reg:squarederror',\n",
    "    num_round=100,\n",
    "    max_depth=6,\n",
    "    eta=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "print(\"Built-in XGBoost estimator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Basic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train initial model\n",
    "if 'train_path' not in locals():\n",
    "    raise NameError(\"train_path not defined. Please run the data preparation cell first.\")\n",
    "\n",
    "print(f\"Starting training with data: {train_path}\")\n",
    "xgb_estimator.fit({\n",
    "    'train': sagemaker.inputs.TrainingInput(train_path, content_type='text/csv')\n",
    "})\n",
    "print(f\"Model training completed: {xgb_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "hyperparameter_ranges = {\n",
    "    'max_depth': IntegerParameter(3, 10),\n",
    "    'eta': ContinuousParameter(0.01, 0.3),\n",
    "    'subsample': ContinuousParameter(0.5, 1.0),\n",
    "    'colsample_bytree': ContinuousParameter(0.5, 1.0),\n",
    "    'num_round': IntegerParameter(50, 200)\n",
    "}\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    xgb_estimator,\n",
    "    objective_metric_name='validation:rmse',\n",
    "    objective_type='Minimize',\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=5,\n",
    "    max_parallel_jobs=2\n",
    ")\n",
    "\n",
    "tuner.fit({\n",
    "    'train': sagemaker.inputs.TrainingInput(train_path, content_type='text/csv'),\n",
    "    'validation': sagemaker.inputs.TrainingInput(test_path, content_type='text/csv')\n",
    "})\n",
    "best_estimator = tuner.best_estimator()\n",
    "print(f\"Best model: {best_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Inference\n",
    "\n",
    "Using the best model from tuning, we'll generate predictions on the Customer Data.\n",
    "\n",
    "This step can take up to 8 minutes depending on the instance type you choose for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch inference\n",
    "batch_input = test_data[feature_cols].fillna(0).astype(float)\n",
    "batch_input.to_csv('notebook_outputs/batch_input.csv', index=False, header=False)\n",
    "batch_input_path = sagemaker_session.upload_data('notebook_outputs/batch_input.csv', bucket, f'{prefix}/batch/input')\n",
    "\n",
    "# Use built-in XGBoost container for batch transform\n",
    "from sagemaker.model import Model\n",
    "\n",
    "# Create model from best estimator\n",
    "model = Model(\n",
    "    image_uri=best_estimator.image_uri,\n",
    "    model_data=best_estimator.model_data,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Create transformer\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=f's3://{bucket}/{prefix}/batch/output/'\n",
    ")\n",
    "\n",
    "print(f\"Starting batch transform with model: {best_estimator.model_data}\")\n",
    "transformer.transform(batch_input_path, content_type='text/csv')\n",
    "transformer.wait()\n",
    "print(\"Batch inference completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and analyze predictions\n",
    "import boto3\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Get prediction file\n",
    "output_key = f'{prefix}/batch/output/batch_input.csv.out'\n",
    "s3_client.download_file(bucket, output_key, 'notebook_outputs/predictions.csv')\n",
    "\n",
    "# Load predictions\n",
    "predictions = pd.read_csv('notebook_outputs/predictions.csv', header=None)\n",
    "predictions.columns = ['Predicted_Revenue']\n",
    "\n",
    "# Compare with actual values\n",
    "actual_values = test_data['Revenue'].values\n",
    "predicted_values = predictions['Predicted_Revenue'].values\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(actual_values, predicted_values)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(actual_values, predicted_values)\n",
    "\n",
    "print(f\"Model Performance Metrics:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': actual_values,\n",
    "    'Predicted': predicted_values,\n",
    "    'Error': actual_values - predicted_values\n",
    "})\n",
    "\n",
    "print(\"\\nPrediction Results:\")\n",
    "print(results_df)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df.index, results_df['Actual'], 'o-', label='Actual', linewidth=2)\n",
    "plt.plot(results_df.index, results_df['Predicted'], 's-', label='Predicted', linewidth=2)\n",
    "plt.xlabel('Test Sample')\n",
    "plt.ylabel('Revenue')\n",
    "plt.title('Actual vs Predicted Revenue')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "### ðŸš€ **How to Improve Prediction Accuracy**\n",
    "\n",
    "1. **Larger Training Dataset**: Use full historical data (months/years vs. sample data)\n",
    "2. **Advanced Features**: Add external factors (holidays, promotions, weather)\n",
    "3. **Algorithm Selection**: Try DeepAR, Prophet, or ensemble methods\n",
    "4. **Data Quality**: Handle missing values and remove outliers\n",
    "\n",
    "### âš¡ **How to Improve Performance & Speed**\n",
    "\n",
    "1. **Infrastructure**: Use larger instances (ml.m5.xlarge) and multi-instance training\n",
    "2. **Optimization**: Enable early stopping and cache preprocessed data\n",
    "3. **Real-time**: Deploy to SageMaker endpoints with auto-scaling\n",
    "\n",
    "### ðŸŽ¯ **Next Steps for Production**\n",
    "\n",
    "1. **MLOps**: Set up automated retraining with SageMaker Pipelines\n",
    "2. **Monitoring**: Implement data drift detection\n",
    "3. **Integration**: Connect with business systems (ERP, inventory)\n",
    "\n",
    "### ðŸ’¡ **Business Impact**\n",
    "\n",
    "Accurate forecasting enables inventory optimization, better resource planning, improved financial planning, and higher customer satisfaction through product availability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Resource Clean up\n",
    "\n",
    "Clean up resources to avoid ongoing charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "print('Cleaning up resources created by this notebook...')\n",
    "\n",
    "# Clean up S3 objects created by this notebook\n",
    "print('Deleting S3 objects created by this notebook...')\n",
    "objects_to_delete = [\n",
    "    f'{prefix}/data/train.csv',\n",
    "    f'{prefix}/data/test.csv',\n",
    "    f'{prefix}/batch/input/batch_input.csv',\n",
    "    f'{prefix}/batch/output/batch_input.csv.out'\n",
    "]\n",
    "\n",
    "deleted_count = 0\n",
    "for obj_key in objects_to_delete:\n",
    "    try:\n",
    "        s3_client.delete_object(Bucket=bucket, Key=obj_key)\n",
    "        print(f'Deleted s3://{bucket}/{obj_key}')\n",
    "        deleted_count += 1\n",
    "    except Exception as e:\n",
    "        print(f'Could not delete {obj_key}: {e}')\n",
    "\n",
    "print(f'Deleted {deleted_count} S3 objects')\n",
    "\n",
    "# Clean up local files created by this notebook\n",
    "files_to_delete = ['train.csv', 'test.csv', 'batch_input.csv', 'predictions.csv']\n",
    "for file in files_to_delete:\n",
    "    file_path = os.path.join('notebook_outputs', file)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f'Deleted {file_path}')\n",
    "\n",
    "print('Cleanup completed - only resources created by this notebook were removed!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
